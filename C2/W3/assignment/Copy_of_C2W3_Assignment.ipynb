{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acafb559",
   "metadata": {
    "id": "acafb559"
   },
   "source": [
    "# Week 3: Transfer Learning\n",
    "\n",
    "Welcome to this assignment! This week, you are going to use a technique called `Transfer Learning` in which you utilize an already trained network to help you solve a similar problem to the one it was originally trained to solve.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dcf3a1",
   "metadata": {
    "id": "43dcf3a1"
   },
   "source": [
    "_**NOTE:** To prevent errors from the autograder, pleave avoid editing or deleting non-graded cells in this notebook . Please only put your solutions in between the `### START CODE HERE` and `### END CODE HERE` code comments, and refrain from adding any new cells._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b877d6c9",
   "metadata": {
    "id": "b877d6c9",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# grader-required-cell\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install kaggle"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lLFFqrBU0ng3",
    "outputId": "3d2ae660-bbf0-4978-ad99-0235442931c5"
   },
   "id": "lLFFqrBU0ng3",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!kaggle competitions download -c dogs-vs-cats-redux-kernels-edition -p /tmp/kaggle/dogs-vs-cats"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4JIWAh-I0qaj",
    "outputId": "cde52459-d53c-4725-ab81-64c6b04d7e02"
   },
   "id": "4JIWAh-I0qaj",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ca6f2970",
   "metadata": {
    "id": "ca6f2970"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "For this assignment, you will use the `Horse or Human dataset`, which contains images of horses and humans.\n",
    "\n",
    "Download the `training` and `validation` sets by running the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# grader-required-cell\n",
    "\n",
    "import shutil\n",
    "\n",
    "tmp_path = '/tmp'\n",
    "\n",
    "# Define root directory\n",
    "root_dir = f'{tmp_path}/kaggle/dogs-vs-cats/processed'\n",
    "\n",
    "# Empty directory to prevent FileExistsError is the function is run several times\n",
    "if os.path.exists(root_dir):\n",
    "    shutil.rmtree(root_dir)\n",
    "\n",
    "\n",
    "# GRADED FUNCTION: create_train_val_dirs\n",
    "def create_train_val_dirs(root_path):\n",
    "    \"\"\"\n",
    "    Creates directories for the train and test sets\n",
    "\n",
    "    Args:\n",
    "      root_path (string) - the base directory path to create subdirectories from\n",
    "\n",
    "    Returns:\n",
    "      None\n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "\n",
    "    # HINT:\n",
    "    # Use os.makedirs to create your directories with intermediate subdirectories\n",
    "    # Don't hardcode the paths. Use os.path.join to append the new directories to the root_path parameter\n",
    "\n",
    "    os.makedirs(f\"{root_path}/training/cats\", exist_ok=True)\n",
    "    os.makedirs(f\"{root_path}/training/dogs\", exist_ok=True)\n",
    "    os.makedirs(f\"{root_path}/validation/cats\", exist_ok=True)\n",
    "    os.makedirs(f\"{root_path}/validation/dogs\", exist_ok=True)\n",
    "\n",
    "    ### END CODE HERE\n",
    "\n",
    "\n",
    "try:\n",
    "    create_train_val_dirs(root_path=root_dir)\n",
    "except FileExistsError:\n",
    "    print(\"You should not be seeing this since the upper directory is removed beforehand\")"
   ],
   "metadata": {
    "id": "yGjpBMw205f_"
   },
   "id": "yGjpBMw205f_",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# grader-required-cell\n",
    "\n",
    "# Test your create_train_val_dirs function\n",
    "\n",
    "for rootdir, dirs, files in os.walk(root_dir):\n",
    "    for subdir in dirs:\n",
    "        print(os.path.join(rootdir, subdir))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NcBZcXoN1IaW",
    "outputId": "85666d05-f176-4c93-8728-2cf64200487b"
   },
   "id": "NcBZcXoN1IaW",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c0655c",
   "metadata": {
    "id": "25c0655c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Get the Horse or Human training dataset\n",
    "# !wget -q -P /content/ https://storage.googleapis.com/tensorflow-1-public/course2/week3/horse-or-human.zip\n",
    "\n",
    "# # Get the Horse or Human validation dataset\n",
    "# !wget -q -P /content/ https://storage.googleapis.com/tensorflow-1-public/course2/week3/validation-horse-or-human.zip\n",
    "\n",
    "# test_local_zip = './horse-or-human.zip'\n",
    "# zip_ref = zipfile.ZipFile(test_local_zip, 'r')\n",
    "# zip_ref.extractall('/tmp/training')\n",
    "\n",
    "# val_local_zip = './validation-horse-or-human.zip'\n",
    "# zip_ref = zipfile.ZipFile(val_local_zip, 'r')\n",
    "# zip_ref.extractall('/tmp/validation')\n",
    "\n",
    "# zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370dc5b1",
   "metadata": {
    "id": "370dc5b1"
   },
   "source": [
    "This dataset already has an structure that is compatible with Keras' `flow_from_directory` so you don't need to move the images into subdirectories as you did in the previous assignments. However, it is still a good idea to save the paths of the images so you can use them later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b64500f",
   "metadata": {
    "id": "2b64500f",
    "lines_to_next_cell": 2,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# # grader-required-cell\n",
    "\n",
    "# # Define the training and validation base directories\n",
    "# train_dir = '/tmp/training'\n",
    "# validation_dir = '/tmp/validation'\n",
    "\n",
    "# # Directory with training horse pictures\n",
    "# train_horses_dir = os.path.join(train_dir, 'horses')\n",
    "# # Directory with training humans pictures\n",
    "# train_humans_dir = os.path.join(train_dir, 'humans')\n",
    "# # Directory with validation horse pictures\n",
    "# validation_horses_dir = os.path.join(validation_dir, 'horses')\n",
    "# # Directory with validation human pictures\n",
    "# validation_humans_dir = os.path.join(validation_dir, 'humans')\n",
    "\n",
    "# # Check the number of images for each class and set\n",
    "# print(f\"There are {len(os.listdir(train_horses_dir))} images of horses for training.\\n\")\n",
    "# print(f\"There are {len(os.listdir(train_humans_dir))} images of humans for training.\\n\")\n",
    "# print(f\"There are {len(os.listdir(validation_horses_dir))} images of horses for validation.\\n\")\n",
    "# print(f\"There are {len(os.listdir(validation_humans_dir))} images of humans for validation.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bd474d",
   "metadata": {
    "id": "e8bd474d"
   },
   "source": [
    "Now take a look at a sample image of each one of the classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44e2839",
   "metadata": {
    "id": "d44e2839",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# # grader-required-cell\n",
    "\n",
    "# print(\"Sample horse image:\")\n",
    "# plt.imshow(load_img(f\"{os.path.join(train_horses_dir, os.listdir(train_horses_dir)[0])}\"))\n",
    "# plt.show()\n",
    "\n",
    "# print(\"\\nSample human image:\")\n",
    "# plt.imshow(load_img(f\"{os.path.join(train_humans_dir, os.listdir(train_humans_dir)[0])}\"))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864d4d2d",
   "metadata": {
    "id": "864d4d2d"
   },
   "source": [
    "`matplotlib` makes it easy to see that these images have a resolution of 300x300 and are colored, but you can double check this by using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03363795",
   "metadata": {
    "id": "03363795",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# # grader-required-cell\n",
    "\n",
    "# # Load the first example of a horse\n",
    "# sample_image  = load_img(f\"{os.path.join(train_horses_dir, os.listdir(train_horses_dir)[0])}\")\n",
    "\n",
    "# # Convert the image into its numpy array representation\n",
    "# sample_array = img_to_array(sample_image)\n",
    "\n",
    "# print(f\"Each image has shape: {sample_array.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!unzip -o /tmp/kaggle/dogs-vs-cats/dogs-vs-cats-redux-kernels-edition.zip -d /tmp/kaggle/dogs-vs-cats\n",
    "!unzip -o /tmp/kaggle/dogs-vs-cats/train.zip -d /tmp/kaggle/dogs-vs-cats\n",
    "!unzip -o /tmp/kaggle/dogs-vs-cats/test.zip -d /tmp/kaggle/dogs-vs-cats"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ArPzcl_B2fVh",
    "outputId": "00d0932b-dec6-4a41-8be4-38c1c2df65be"
   },
   "id": "ArPzcl_B2fVh",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# grader-required-cell\n",
    "\n",
    "# GRADED FUNCTION: split_data\n",
    "def split_data(SOURCE_DIR, FILENAME_GLOB_PATTERN, TRAINING_DIR, VALIDATION_DIR, SPLIT_SIZE):\n",
    "    \"\"\"\n",
    "    Splits the data into train and test sets\n",
    "\n",
    "    Args:\n",
    "      SOURCE_DIR (string): directory path containing the images\n",
    "      TRAINING_DIR (string): directory path to be used for training\n",
    "      VALIDATION_DIR (string): directory path to be used for validation\n",
    "      SPLIT_SIZE (float): proportion of the dataset to be used for training\n",
    "\n",
    "    Returns:\n",
    "      None\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE\n",
    "    from pathlib import Path\n",
    "    filenames = [os.path.basename(p) for p in Path(SOURCE_DIR).glob(FILENAME_GLOB_PATTERN)]\n",
    "    # filenames = os.listdir(SOURCE_DIR)\n",
    "    for f in [f for f in filenames if os.path.getsize(f\"{SOURCE_DIR}/{f}\") == 0]:\n",
    "        print(f\"{f} is zero length, so ignoring.\")\n",
    "    filenames = [f for f in filenames if os.path.getsize(f\"{SOURCE_DIR}/{f}\") > 0]\n",
    "    filenames = random.sample(filenames, len(filenames))\n",
    "    training_file_count = int(len(filenames) * SPLIT_SIZE)\n",
    "    validation_file_count = len(filenames) - training_file_count\n",
    "    for f in filenames[:training_file_count]:\n",
    "        shutil.copyfile(f\"{SOURCE_DIR}/{f}\", f\"{TRAINING_DIR}/{f}\")\n",
    "    for f in filenames[:validation_file_count]:\n",
    "        shutil.copyfile(f\"{SOURCE_DIR}/{f}\", f\"{VALIDATION_DIR}/{f}\")\n",
    "    ### END CODE HERE"
   ],
   "metadata": {
    "id": "ZUnnMD1X1Rmx"
   },
   "id": "ZUnnMD1X1Rmx",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# grader-required-cell\n",
    "\n",
    "import random\n",
    "\n",
    "# Test your split_data function\n",
    "\n",
    "# Define paths\n",
    "# CAT_SOURCE_DIR = f\"{tmp_path}/PetImages/Cat/\"\n",
    "# DOG_SOURCE_DIR = f\"{tmp_path}/PetImages/Dog/\"\n",
    "SOURCE_DIR = f\"{tmp_path}/kaggle/dogs-vs-cats/train/\"\n",
    "\n",
    "# TRAINING_DIR = f\"{tmp_path}/cats-v-dogs/training/\"\n",
    "# VALIDATION_DIR = f\"{tmp_path}/cats-v-dogs/validation/\"\n",
    "TRAINING_DIR = f\"{tmp_path}/kaggle/dogs-vs-cats/processed/training/\"\n",
    "VALIDATION_DIR = f\"{tmp_path}/kaggle/dogs-vs-cats/processed/validation/\"\n",
    "\n",
    "TRAINING_CATS_DIR = os.path.join(TRAINING_DIR, \"cats/\")\n",
    "VALIDATION_CATS_DIR = os.path.join(VALIDATION_DIR, \"cats/\")\n",
    "\n",
    "TRAINING_DOGS_DIR = os.path.join(TRAINING_DIR, \"dogs/\")\n",
    "VALIDATION_DOGS_DIR = os.path.join(VALIDATION_DIR, \"dogs/\")\n",
    "\n",
    "# Empty directories in case you run this cell multiple times\n",
    "if len(os.listdir(TRAINING_CATS_DIR)) > 0:\n",
    "    for file in os.scandir(TRAINING_CATS_DIR):\n",
    "        os.remove(file.path)\n",
    "if len(os.listdir(TRAINING_DOGS_DIR)) > 0:\n",
    "    for file in os.scandir(TRAINING_DOGS_DIR):\n",
    "        os.remove(file.path)\n",
    "if len(os.listdir(VALIDATION_CATS_DIR)) > 0:\n",
    "    for file in os.scandir(VALIDATION_CATS_DIR):\n",
    "        os.remove(file.path)\n",
    "if len(os.listdir(VALIDATION_DOGS_DIR)) > 0:\n",
    "    for file in os.scandir(VALIDATION_DOGS_DIR):\n",
    "        os.remove(file.path)\n",
    "\n",
    "# Define proportion of images used for training\n",
    "split_size = .9\n",
    "\n",
    "# Run the function\n",
    "# NOTE: Messages about zero length images should be printed out\n",
    "# split_data(CAT_SOURCE_DIR, TRAINING_CATS_DIR, VALIDATION_CATS_DIR, split_size)\n",
    "# split_data(DOG_SOURCE_DIR, TRAINING_DOGS_DIR, VALIDATION_DOGS_DIR, split_size)\n",
    "split_data(SOURCE_DIR, \"cat.*.jpg\", TRAINING_CATS_DIR, VALIDATION_CATS_DIR, split_size)\n",
    "split_data(SOURCE_DIR, \"dog.*.jpg\", TRAINING_DOGS_DIR, VALIDATION_DOGS_DIR, split_size)\n",
    "\n",
    "# Check that the number of images matches the expected output\n",
    "\n",
    "# Your function should perform copies rather than moving images so original directories should contain unchanged images\n",
    "# print(f\"\\n\\nOriginal cat's directory has {len(os.listdir(CAT_SOURCE_DIR))} images\")\n",
    "# print(f\"Original dog's directory has {len(os.listdir(DOG_SOURCE_DIR))} images\\n\")\n",
    "\n",
    "# Training and validation splits\n",
    "print(f\"There are {len(os.listdir(TRAINING_CATS_DIR))} images of cats for training\")\n",
    "print(f\"There are {len(os.listdir(TRAINING_DOGS_DIR))} images of dogs for training\")\n",
    "print(f\"There are {len(os.listdir(VALIDATION_CATS_DIR))} images of cats for validation\")\n",
    "print(f\"There are {len(os.listdir(VALIDATION_DOGS_DIR))} images of dogs for validation\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_GE_N6dh1Y6O",
    "outputId": "03d06b2d-dd71-4b47-b6ba-a0078560fc8a"
   },
   "id": "_GE_N6dh1Y6O",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1e221e73",
   "metadata": {
    "id": "1e221e73"
   },
   "source": [
    "As expected, the sample image has a resolution of 300x300 and the last dimension is used for each one of the RGB channels to represent color."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e6443c",
   "metadata": {
    "id": "57e6443c"
   },
   "source": [
    "## Training and Validation Generators\n",
    "\n",
    "Now that you know the images you are dealing with, it is time for you to code the generators that will fed these images to your Network. For this, complete the `train_val_generators` function below:\n",
    "\n",
    "**Important Note:** The images have a resolution of 300x300 but the `flow_from_directory` method you will use allows you to set a target resolution. In this case, **set a `target_size` of (150, 150)**. This will heavily lower the number of trainable parameters in your final network, yielding much quicker training times without compromising the accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1e8b2a",
   "metadata": {
    "cellView": "code",
    "id": "3e1e8b2a",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# grader-required-cell\n",
    "\n",
    "# GRADED FUNCTION: train_val_generators\n",
    "def train_val_generators(TRAINING_DIR, VALIDATION_DIR):\n",
    "  \"\"\"\n",
    "  Creates the training and validation data generators\n",
    "\n",
    "  Args:\n",
    "    TRAINING_DIR (string): directory path containing the training images\n",
    "    VALIDATION_DIR (string): directory path containing the testing/validation images\n",
    "\n",
    "  Returns:\n",
    "    train_generator, validation_generator: tuple containing the generators\n",
    "  \"\"\"\n",
    "  ### START CODE HERE\n",
    "\n",
    "  # Instantiate the ImageDataGenerator class\n",
    "  # Don't forget to normalize pixel values and set arguments to augment the images\n",
    "  # train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "  #                                     # rotation_range=40,\n",
    "  #                                     # width_shift_range=0.2,\n",
    "  #                                     # height_shift_range=0.2,\n",
    "  #                                     # shear_range=0.2,\n",
    "  #                                     # zoom_range=0.2,\n",
    "  #                                     # horizontal_flip=True,\n",
    "  #                                     # fill_mode='nearest',\n",
    "  #                                     )\n",
    "\n",
    "  # Pass in the appropriate arguments to the flow_from_directory method\n",
    "  # train_generator = train_datagen.flow_from_directory(directory=TRAINING_DIR,\n",
    "  #                                                     batch_size=256,\n",
    "  #                                                     class_mode='binary',\n",
    "  #                                                     target_size=(300, 300))\n",
    "  train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    TRAINING_DIR,\n",
    "    label_mode='binary',\n",
    "    batch_size=32,\n",
    "    image_size=(300, 300),\n",
    "    shuffle=True,\n",
    "  )\n",
    "  validation_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    VALIDATION_DIR,\n",
    "    label_mode='binary',\n",
    "    batch_size=32,\n",
    "    image_size=(300, 300),\n",
    "    shuffle=True,\n",
    "  )\n",
    "\n",
    "  ### END CODE HERE\n",
    "  return train_dataset, validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432519f0",
   "metadata": {
    "id": "432519f0",
    "tags": [
     "graded"
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "99ee515a-a0d6-46eb-b93d-e87af3089a20"
   },
   "outputs": [],
   "source": [
    "# grader-required-cell\n",
    "\n",
    "# Test your generators\n",
    "train_dataset, validation_dataset = train_val_generators(TRAINING_DIR, VALIDATION_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637c723b",
   "metadata": {
    "id": "637c723b"
   },
   "source": [
    "**Expected Output:**\n",
    "```\n",
    "Found 1027 images belonging to 2 classes.\n",
    "Found 256 images belonging to 2 classes.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4dbb1c",
   "metadata": {
    "id": "4d4dbb1c"
   },
   "source": [
    "## Transfer learning - Create the pre-trained model\n",
    "\n",
    "Download the `inception V3` weights into the `/tmp/` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee1917a",
   "metadata": {
    "id": "bee1917a",
    "tags": [],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9427375f-8215-471a-e5ce-093198d8e9dc"
   },
   "outputs": [],
   "source": [
    "# Download the inception v3 weights\n",
    "!wget --no-check-certificate \\\n",
    "    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
    "    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e68079",
   "metadata": {
    "id": "f7e68079"
   },
   "source": [
    "Now load the `InceptionV3` model and save the path to the weights you just downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990e818b",
   "metadata": {
    "id": "990e818b",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# grader-required-cell\n",
    "\n",
    "# Import the inception model\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "# Create an instance of the inception model from the local pre-trained weights\n",
    "local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6941db",
   "metadata": {
    "id": "0e6941db"
   },
   "source": [
    "Complete the `create_pre_trained_model` function below. You should specify the correct `input_shape` for the model (remember that you set a new resolution for the images instead of the native 300x300) and make all of the layers non-trainable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014e3bbc",
   "metadata": {
    "cellView": "code",
    "id": "014e3bbc",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# grader-required-cell\n",
    "\n",
    "# GRADED FUNCTION: create_pre_trained_model\n",
    "def create_pre_trained_model(local_weights_file):\n",
    "  \"\"\"\n",
    "  Initializes an InceptionV3 model.\n",
    "\n",
    "  Args:\n",
    "    local_weights_file (string): path pointing to a pretrained weights H5 file\n",
    "\n",
    "  Returns:\n",
    "    pre_trained_model: the initialized InceptionV3 model\n",
    "  \"\"\"\n",
    "  ### START CODE HERE\n",
    "\n",
    "  layer = layers.Input((300, 300, 3))\n",
    "  layer = layers.Rescaling(1./255)(layer)\n",
    "  layer = layers.RandomFlip()(layer)\n",
    "  layer = layers.RandomTranslation(width_factor=(-0.2, 0.2), height_factor=(-0.2, 0.2))(layer)\n",
    "  layer = layers.RandomRotation((-0.4, 0.4))(layer)\n",
    "  layer = layers.RandomZoom(0.2)(layer)\n",
    "  layer = layers.RandomContrast(0.2)(layer)\n",
    "  layer = layers.RandomBrightness(0.2, value_range=[0.0, 1.0])(layer)\n",
    "  input_processing_end = layers.Layer(name=\"input_processing_end\")\n",
    "\n",
    "  pre_trained_model = InceptionV3(\n",
    "    input_tensor = input_processing_end(layer),\n",
    "    # input_shape = (300,300,3),\n",
    "    include_top = False,\n",
    "    weights = None\n",
    "  )\n",
    "\n",
    "  pre_trained_model.load_weights(local_weights_file)\n",
    "\n",
    "  # Make all the layers in the pre-trained model non-trainable\n",
    "  reached_input_processing_end = False\n",
    "  for layer in pre_trained_model.layers:\n",
    "    if layer.name == input_processing_end.name:\n",
    "      reached_input_processing_end = True\n",
    "    if reached_input_processing_end:\n",
    "      layer.trainable = False\n",
    "\n",
    "  ### END CODE HERE\n",
    "\n",
    "  return pre_trained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e4d059",
   "metadata": {
    "id": "00e4d059"
   },
   "source": [
    "Check that everything went well by comparing the last few rows of the model summary to the expected output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8c076c",
   "metadata": {
    "id": "1d8c076c",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# grader-required-cell\n",
    "\n",
    "pre_trained_model = create_pre_trained_model(local_weights_file)\n",
    "\n",
    "# Print the model summary\n",
    "# pre_trained_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50749676",
   "metadata": {
    "id": "50749676"
   },
   "source": [
    "To check that all the layers in the model were set to be non-trainable, you can also run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6768411a",
   "metadata": {
    "id": "6768411a",
    "tags": [
     "graded"
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2782f336-fda2-4227-bbba-3677b3181b03"
   },
   "outputs": [],
   "source": [
    "# grader-required-cell\n",
    "\n",
    "total_params = pre_trained_model.count_params()\n",
    "num_trainable_params = sum([w.shape.num_elements() for w in pre_trained_model.trainable_weights])\n",
    "\n",
    "print(f\"There are {total_params:,} total parameters in this model.\")\n",
    "print(f\"There are {num_trainable_params:,} trainable parameters in this model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f39763",
   "metadata": {
    "id": "d8f39763"
   },
   "source": [
    "**Expected Output:**\n",
    "```\n",
    "There are 21,802,784 total parameters in this model.\n",
    "There are 0 trainable parameters in this model.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f850d5fd",
   "metadata": {
    "id": "f850d5fd"
   },
   "source": [
    "## Creating callbacks for later\n",
    "\n",
    "You have already worked with callbacks in the first course of this specialization so the callback to stop training once an accuracy of 99.9% is reached, is provided for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba844313",
   "metadata": {
    "id": "ba844313",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# grader-required-cell\n",
    "\n",
    "# Define a Callback class that stops training once accuracy reaches 99.9%\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    if(logs.get('accuracy')>0.999999):\n",
    "      print(\"\\nReached 99.9999% accuracy so cancelling training!\")\n",
    "      self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2f8ea6",
   "metadata": {
    "id": "4f2f8ea6"
   },
   "source": [
    "## Pipelining the pre-trained model with your own\n",
    "\n",
    "Now that the pre-trained model is ready, you need to \"glue\" it to your own model to solve the task at hand.\n",
    "\n",
    "For this you will need the last output of the pre-trained model, since this will be the input for your own. Complete the `output_of_last_layer` function below.\n",
    "\n",
    "**Note:** For grading purposes use the `mixed7` layer as the last layer of the pre-trained model. However, after submitting feel free to come back here and play around with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5080f870",
   "metadata": {
    "id": "5080f870",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# grader-required-cell\n",
    "\n",
    "# GRADED FUNCTION: output_of_last_layer\n",
    "def output_of_last_layer(pre_trained_model):\n",
    "  \"\"\"\n",
    "  Gets the last layer output of a model\n",
    "\n",
    "  Args:\n",
    "    pre_trained_model (tf.keras Model): model to get the last layer output from\n",
    "\n",
    "  Returns:\n",
    "    last_output: output of the model's last layer\n",
    "  \"\"\"\n",
    "  ### START CODE HERE\n",
    "  last_desired_layer = pre_trained_model.get_layer('mixed9')\n",
    "  print('last layer output shape: ', last_desired_layer.output_shape)\n",
    "  last_output = last_desired_layer.output\n",
    "  print('last layer output: ', last_output)\n",
    "  ### END CODE HERE\n",
    "\n",
    "  return last_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc01da83",
   "metadata": {
    "id": "fc01da83"
   },
   "source": [
    "Check that everything works as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73882807",
   "metadata": {
    "id": "73882807",
    "tags": [
     "graded"
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "200d56f5-29b6-431d-fabb-6126d4418f5a"
   },
   "outputs": [],
   "source": [
    "# grader-required-cell\n",
    "\n",
    "last_output = output_of_last_layer(pre_trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e24ec6",
   "metadata": {
    "id": "97e24ec6"
   },
   "source": [
    "**Expected Output (if `mixed7` layer was used):**\n",
    "```\n",
    "last layer output shape:  (None, 7, 7, 768)\n",
    "last layer output:  KerasTensor(type_spec=TensorSpec(shape=(None, 7, 7, 768), dtype=tf.float32, name=None), name='mixed7/concat:0', description=\"created by layer 'mixed7'\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ea0c32",
   "metadata": {
    "id": "e4ea0c32"
   },
   "source": [
    "Now you will create the final model by adding some additional layers on top of the pre-trained model.\n",
    "\n",
    "Complete the `create_final_model` function below. You will need to use Tensorflow's [Functional API](https://www.tensorflow.org/guide/keras/functional) for this since the pretrained model has been created using it.\n",
    "\n",
    "Let's double check this first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdb86a7",
   "metadata": {
    "id": "cfdb86a7",
    "tags": [
     "graded"
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e21068ba-150d-4631-84df-f1e154990a87"
   },
   "outputs": [],
   "source": [
    "# grader-required-cell\n",
    "\n",
    "# Print the type of the pre-trained model\n",
    "print(f\"The pretrained model has type: {type(pre_trained_model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46929f28",
   "metadata": {
    "id": "46929f28"
   },
   "source": [
    "To create the final model, you will use Keras' Model class by defining the appropriate inputs and outputs as described in the first way to instantiate a Model in the [docs](https://www.tensorflow.org/api_docs/python/tf/keras/Model).\n",
    "\n",
    "Note that you can get the input from any existing model by using its `input` attribute and by using the Funcional API you can use the last layer directly as output when creating the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc96239",
   "metadata": {
    "cellView": "code",
    "id": "2bc96239",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# grader-required-cell\n",
    "\n",
    "# GRADED FUNCTION: create_final_model\n",
    "def create_final_model(pre_trained_model, last_output):\n",
    "  \"\"\"\n",
    "  Appends a custom model to a pre-trained model\n",
    "\n",
    "  Args:\n",
    "    pre_trained_model (tf.keras Model): model that will accept the train/test inputs\n",
    "    last_output (tensor): last layer output of the pre-trained model\n",
    "\n",
    "  Returns:\n",
    "    model: the combined model\n",
    "  \"\"\"\n",
    "  from tensorflow.keras import layers\n",
    "\n",
    "  # Flatten the output layer to 1 dimension\n",
    "  x = layers.Flatten()(last_output)\n",
    "\n",
    "  ### START CODE HERE\n",
    "\n",
    "  # Add a fully connected layer with 1024 hidden units and ReLU activation\n",
    "  x = layers.Dense(1024, activation='relu')(x)\n",
    "  # Add a dropout rate of 0.2\n",
    "  x = layers.Dropout(0.2)(x)\n",
    "  # Add a final sigmoid layer for classification\n",
    "  x = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "  # Create the complete model by using the Model class\n",
    "  model = Model(inputs=pre_trained_model.input, outputs=x)\n",
    "\n",
    "  # Compile the model\n",
    "  model.compile(optimizer = RMSprop(learning_rate=0.0001),\n",
    "                loss = 'binary_crossentropy',\n",
    "                metrics = ['accuracy'])\n",
    "\n",
    "  ### END CODE HERE\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afef86f0",
   "metadata": {
    "id": "afef86f0",
    "tags": [
     "graded"
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0505e386-1df2-4891-8050-a918fa001a8c"
   },
   "outputs": [],
   "source": [
    "# grader-required-cell\n",
    "\n",
    "# Save your model in a variable\n",
    "model = create_final_model(pre_trained_model, last_output)\n",
    "\n",
    "# Inspect parameters\n",
    "total_params = model.count_params()\n",
    "num_trainable_params = sum([w.shape.num_elements() for w in model.trainable_weights])\n",
    "\n",
    "print(f\"There are {total_params:,} total parameters in this model.\")\n",
    "print(f\"There are {num_trainable_params:,} trainable parameters in this model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e856c752",
   "metadata": {
    "id": "e856c752"
   },
   "source": [
    "**Expected Output:**\n",
    "```\n",
    "There are 47,512,481 total parameters in this model.\n",
    "There are 38,537,217 trainable parameters in this model.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c13772",
   "metadata": {
    "id": "a1c13772"
   },
   "source": [
    "Wow, that is a lot of parameters!\n",
    "\n",
    "After submitting your assignment later, try re-running this notebook but use the original resolution of 300x300, you will be surprised to see how many more parameters are for that case.\n",
    "\n",
    "Now train the model:"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"model_checkpoint_2\",\n",
    "    monitor = 'val_accuracy',\n",
    "    save_best_only = True,\n",
    ")"
   ],
   "metadata": {
    "id": "PKAuG6rv7ozY"
   },
   "id": "PKAuG6rv7ozY",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6644c2",
   "metadata": {
    "id": "2e6644c2",
    "tags": [],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "623459f3-eb52-4979-9404-429f0e3b0b62"
   },
   "outputs": [],
   "source": [
    "# Run this and see how many epochs it should take before the callback\n",
    "# fires, and stops training at 99.9% accuracy\n",
    "# (It should take a few epochs)\n",
    "callback = myCallback()\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    validation_data = validation_dataset,\n",
    "                    epochs = 10,\n",
    "                    # verbose = 2,\n",
    "                    callbacks=[callback, checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "len(train_dataset)"
   ],
   "metadata": {
    "id": "2fpFEXUoHa_O",
    "outputId": "f20172b6-3b29-4988-cf22-c6fb4afe7e2e",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "2fpFEXUoHa_O",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8f450fb5",
   "metadata": {
    "id": "8f450fb5"
   },
   "source": [
    "The training should have stopped after less than 10 epochs and it should have reached an accuracy over 99,9% (firing the callback). This happened so quickly because of the pre-trained model you used, which already contained information to classify humans from horses. Really cool!\n",
    "\n",
    "Now take a quick look at the training and validation accuracies for each epoch of training:"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install natsort\n"
   ],
   "metadata": {
    "id": "7t9J75cRA5BC",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e833add6-3e08-43ee-9db4-ddaa98fedf08"
   },
   "id": "7t9J75cRA5BC",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# !mkdir -p /tmp/kaggle/dogs-vs-cats/test_tmp\n",
    "# !cp /tmp/kaggle/dogs-vs-cats/test/1.jpg /tmp/kaggle/dogs-vs-cats/test_tmp/\n",
    "# !cp /tmp/kaggle/dogs-vs-cats/test/2.jpg /tmp/kaggle/dogs-vs-cats/test_tmp/\n",
    "# !cp /tmp/kaggle/dogs-vs-cats/test/10.jpg /tmp/kaggle/dogs-vs-cats/test_tmp/\n",
    "# !cp /tmp/kaggle/dogs-vs-cats/test/11.jpg /tmp/kaggle/dogs-vs-cats/test_tmp/\n",
    "# !cp /tmp/kaggle/dogs-vs-cats/test/100.jpg /tmp/kaggle/dogs-vs-cats/test_tmp/\n",
    "# !cp /tmp/kaggle/dogs-vs-cats/test/101.jpg /tmp/kaggle/dogs-vs-cats/test_tmp/"
   ],
   "metadata": {
    "id": "-hM8nRUoLw5U"
   },
   "id": "-hM8nRUoLw5U",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from natsort import natsorted\n",
    "import numpy as np\n",
    "\n",
    "TEST_DIR = f\"{tmp_path}/kaggle/dogs-vs-cats/test\"\n",
    "\n",
    "test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "  TEST_DIR,\n",
    "  label_mode=None,\n",
    "  batch_size=32,\n",
    "  image_size=(300, 300),\n",
    "  shuffle=False,\n",
    ")\n",
    "\n",
    "predictions = model.predict(test_dataset, batch_size=32).squeeze()\n",
    "print(predictions[:10])"
   ],
   "metadata": {
    "id": "jPx5PooRA6Ml",
    "outputId": "302f0f04-0e43-41e7-ce52-c31be96ba5e3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "jPx5PooRA6Ml",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "predictions_sorted = np.asarray(natsorted(list(zip(test_dataset.file_paths, predictions))))[:, 1]\n",
    "print(predictions_sorted[:10])"
   ],
   "metadata": {
    "id": "ATufhF2zOC58",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5def3363-9fec-4fa9-f35e-c33cd03175be"
   },
   "id": "ATufhF2zOC58",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame(predictions_sorted, columns=[\"label\"])\n",
    "df[\"id\"] = df.index + 1\n",
    "df[\"label\"] = np.around(df[\"label\"].astype(float)).astype(int)\n",
    "# df[\"label\"] = df[\"label\"].astype(float).round(3)\n",
    "df.to_csv(f\"{tmp_path}/kaggle/dogs-vs-cats/result-advanced-rounded.csv\", columns=[\"id\", \"label\"], index=False)\n",
    "df.sample(10)"
   ],
   "metadata": {
    "id": "M_77V64oCXy2",
    "outputId": "8bbd1421-32c2-4596-b704-a88f8b3efe76",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    }
   },
   "id": "M_77V64oCXy2",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cb0a4b",
   "metadata": {
    "id": "b0cb0a4b",
    "tags": [],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "outputId": "7b03268d-c81b-46ca-b80b-442327a5c510"
   },
   "outputs": [],
   "source": [
    "# Plot the training and validation accuracies for each epoch\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend(loc=0)\n",
    "plt.figure()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb65110",
   "metadata": {
    "id": "9bb65110"
   },
   "source": [
    "## Download your notebook for grading\n",
    "\n",
    "You will need to submit your solution notebook for grading. The following code cells will check if this notebook's grader metadata (i.e. hidden data in the notebook needed for grading) is not modified by your workspace. This will ensure that the autograder can evaluate your code properly. Depending on its output, you will either:\n",
    "\n",
    "* *if the metadata is intact*: Download the current notebook. Click on the File tab on the upper left corner of the screen then click on `Download -> Download .ipynb.` You can name it anything you want as long as it is a valid `.ipynb` (jupyter notebook) file.\n",
    "<br>\n",
    "\n",
    "* *if the metadata is missing*: A new notebook with your solutions will be created on this Colab workspace. It should be downloaded automatically and you can submit that to the grader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a43c2bb",
   "metadata": {
    "id": "3a43c2bb"
   },
   "outputs": [],
   "source": [
    "# # Download metadata checker\n",
    "# !wget -nc https://storage.googleapis.com/tensorflow-1-public/colab_metadata_checker.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee3e8f9",
   "metadata": {
    "id": "8ee3e8f9"
   },
   "outputs": [],
   "source": [
    "# import colab_metadata_checker\n",
    "\n",
    "# # Please see the output of this cell to see which file you need to submit to the grader\n",
    "# colab_metadata_checker.run('C2W3_Assignment_fixed.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0610fbab",
   "metadata": {
    "id": "0610fbab"
   },
   "source": [
    "**Please disregard the following note if the notebook metadata is detected**\n",
    "\n",
    "_Note: Just in case the download fails for the second point above, you can also do these steps:_\n",
    "* _Click the Folder icon on the left side of this screen to open the File Manager._\n",
    "* _Click the Folder Refresh icon in the File Manager to see the latest files in the workspace. You should see a file ending with a `_fixed.ipynb`._\n",
    "* _Right-click on that file to save locally and submit it to the grader._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba15231",
   "metadata": {
    "id": "8ba15231"
   },
   "source": [
    "**Congratulations on finishing this week's assignment!**\n",
    "\n",
    "You have successfully implemented a convolutional neural network that leverages a pre-trained network to help you solve the problem of classifying humans from horses.\n",
    "\n",
    "**Keep it up!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
